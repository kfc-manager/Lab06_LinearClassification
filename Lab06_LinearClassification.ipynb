{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i0ChBBhxF0K"
      },
      "source": [
        "# Linear Classification\n",
        "\n",
        "In this lab you will implement parts of a linear classification model using the regularized empirical risk minimization principle. By completing this lab and analysing the code, you gain deeper understanding of these type of models, and of gradient descent.\n",
        "\n",
        "\n",
        "## Problem Setting\n",
        "\n",
        "The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each of the patients is classified into two categories: normal (1) and abnormal (0). The training data contains 80 SPECT images from which 22 binary features have been extracted. The goal is to predict the label for an unseen test set of 187 tomography images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "qSGEB3UkxF0L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "testfile = urllib.request.URLopener()\n",
        "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\", \"SPECT.train\")\n",
        "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.test\", \"SPECT.test\")\n",
        "\n",
        "df_train = pd.read_csv('SPECT.train',header=None)\n",
        "df_test = pd.read_csv('SPECT.test',header=None)\n",
        "\n",
        "train = df_train.values\n",
        "test = df_test.values\n",
        "\n",
        "y_train = train[:,0]\n",
        "X_train = train[:,1:]\n",
        "y_test = test[:,0]\n",
        "X_test = test[:,1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBPhAtmexF0N"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Analyze the function learn_reg_ERM(X,y,lambda) which for a given $n\\times m$ data matrix $\\textbf{X}$ and binary class label $\\textbf{y}$ learns and returns a linear model $\\textbf{w}$.\n",
        "The binary class label has to be transformed so that its range is $\\left \\{-1,1 \\right \\}$. \n",
        "The trade-off parameter between the empirical loss and the regularizer is given by $\\lambda > 0$. \n",
        "To adapt the learning rate the Barzilai-Borwein method is used.\n",
        "\n",
        "Try to understand each step of the learning algorithm and comment each line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "1-HgTxIQxF0N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.9998767656955323\n",
            "loss: 0.6021972441121234\n",
            "loss: 0.5508417655792076\n",
            "loss: 0.7613976083001255\n",
            "loss: 0.5280607338838126\n",
            "loss: 0.5332242136094736\n",
            "loss: 0.8094581592380659\n",
            "loss: 0.5327054265524052\n",
            "loss: 0.5343537230207829\n",
            "loss: 0.9199999999999994\n",
            "loss: 0.5361962590045364\n",
            "loss: 0.5393696712791997\n",
            "loss: 0.5457701036153914\n",
            "loss: 0.5505201194016545\n",
            "loss: 0.55387803322199\n",
            "loss: 0.5516037402767451\n",
            "loss: 0.5725\n",
            "loss: 0.5468603280447747\n",
            "loss: 0.5487913396065814\n",
            "loss: 0.5604112785984577\n",
            "loss: 0.5472375848930031\n",
            "loss: 0.550208597813352\n",
            "loss: 0.5498906874502577\n",
            "loss: 0.5511001142848109\n",
            "loss: 0.5498518034737143\n",
            "loss: 0.5503527619608404\n",
            "loss: 0.5501934358887723\n",
            "loss: 0.5494675278578774\n",
            "loss: 0.5502176359418278\n",
            "loss: 0.5497995174578564\n",
            "loss: 0.5498013641828752\n",
            "loss: 0.5498157442924378\n",
            "loss: 0.5499108661757942\n",
            "loss: 0.5497136068276693\n",
            "loss: 0.5497439599264456\n",
            "loss: 0.549748277073886\n",
            "loss: 0.5496959182865101\n",
            "[ 2.60796540e-01 -1.74030466e-18  1.57978804e-02  1.81179980e-01\n",
            "  1.57425361e-01  1.78634344e-01  4.13298955e-01  3.24181015e-01\n",
            "  7.78088007e-02  2.60796540e-01  2.97938496e-01  2.13298955e-01\n",
            "  3.40113988e-01  7.78088007e-02  7.60546174e-02  2.59671170e-01\n",
            "  1.78634344e-01 -8.70152330e-19  3.20126733e-01  2.31865945e-01\n",
            "  5.61472955e-02  4.26464893e-01]\n"
          ]
        }
      ],
      "source": [
        "def learn_reg_ERM(X,y,lbda):\n",
        "    max_iter = 200 #max iteration so we avoid possible infinite loop\n",
        "    e  = 0.001\n",
        "    alpha = 1.\n",
        "\n",
        "    w = np.random.randn(X.shape[1]); #random array with size of m\n",
        "    for k in np.arange(max_iter): #loops through array (from 0 to 199)\n",
        "        h = np.dot(X,w) #multiply X (2-Dimensional array) with w (1-Dimensional array) (computes prediction)\n",
        "        l,lg = loss(h, y) #compute hinge loss and its gradient (h is prediction and y is true label)\n",
        "        print ('loss: {}'.format(np.mean(l))) #print mean of the hinge loss so we can see progress of learning\n",
        "        r,rg = reg(w, lbda) #computes the L_2-regularizer and the gradient of the regularizer function at point w\n",
        "        g = np.dot(X.T,lg) + rg #calculate new gradient from loss and regularizer gradient\n",
        "        if (k > 0): #check if we are in the first iteration of the loop\n",
        "            alpha = alpha * (np.dot(g_old.T,g_old))/(np.dot((g_old - g).T,g_old)) \n",
        "        w = w - alpha * g #adjust w by alpha * gradient\n",
        "        if (np.linalg.norm(alpha * g) < e): #check if normalization of alpha * g smaller than 0.001\n",
        "            break #model is shaped enough\n",
        "        g_old = g #remember gradient from last iteration\n",
        "    return w #return linear model\n",
        "\n",
        "print(learn_reg_ERM(X_train,y_train,5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1CmQjDhxF0O"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Fill in the code for the function loss(h,y) which computes the hinge loss and its gradient. \n",
        "This function takes a given vector $\\textbf{y}$ with the true labels $\\in \\left \\{-1,1\\right \\}$ and a vector $\\textbf{h}$ with the function values of the linear model as inputs. The function returns a vector $\\textbf{l}$ with the hinge loss $\\max(0, 1 − y_{i} h_{i})$ and a vector $\\textbf{g}$ with the gradients of the hinge loss w.r.t $\\textbf{h}$. (Note: The partial derivative of the hinge loss with respect to $\\textbf{h}$  is $g_{i} = −y $ if $l_{i} > 0$, else $g_{i} = 0$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "Vct3IsAYxF0O"
      },
      "outputs": [],
      "source": [
        "def loss(h, y):\n",
        "    ##################\n",
        "    #INSERT CODE HERE#\n",
        "    ##################\n",
        "    l = [] #list to collect values for the hinge loss vector\n",
        "    g = [] #list to collect values for the gradient vector\n",
        "    for i in range(0,h.size): #loop through function values of the linear model\n",
        "        new = max(0,1 - y[i] * h[i]) #hinge loss of this iteration\n",
        "        l.append(new) #add hinge loss to vector\n",
        "        if new > 0: #check if hinge loss is bigger than 0\n",
        "            g.append(-y[i]) \n",
        "        else:\n",
        "            g.append(0)\n",
        "    l = np.array(l) #cast to numpy array\n",
        "    g = np.array(g) #cast to numoy array\n",
        "    return l, g #return hinge loss and gradient vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmdwcNAaxF0P"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Fill in the code for the function reg(w,lambda) which computes the $\\mathcal{L}_2$-regularizer and the gradient of the regularizer function at point $\\textbf{w}$. \n",
        "\n",
        "\n",
        "$$r = \\frac{\\lambda}{2} \\textbf{w}^{T}\\textbf{w}$$\n",
        "\n",
        "$$g = \\lambda \\textbf{w}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "CEsrqBTfxF0P"
      },
      "outputs": [],
      "source": [
        "def reg(w, lbda):\n",
        "    ##################\n",
        "    #INSERT CODE HERE#\n",
        "    ##################\n",
        "    tmp = np.dot(w.T,w) #calculate temporary array\n",
        "    r = np.dot(lbda / 2, tmp) #calculate L_2-regularizer\n",
        "    g = np.dot(lbda, w) #calculate gradient of the regularizer function at point w\n",
        "    return r, g #return regularizer and gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXlyhFPmxF0Q"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Fill in the code for the function predict(w,x) which predicts the class label $y$ for a data point $\\textbf{x}$ or a matrix $X$ of data points (row-wise) for a previously trained linear model $\\textbf{w}$. If there is only a data point given, the function is supposed to return a scalar value. If a matrix is given a vector of predictions is supposed to be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "bnKXor1NxF0Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 2.0842673369284492\n",
            "loss: 2.3903627702383408\n",
            "loss: 0.9116958237267554\n",
            "loss: 0.6143018979159074\n",
            "loss: 0.7119685050629853\n",
            "loss: 0.6894263750417838\n",
            "loss: 0.6993824289316135\n",
            "loss: 0.6968955001377835\n",
            "loss: 0.6980683644873336\n",
            "loss: 0.7026249999999999\n",
            "loss: 0.6923036309136423\n",
            "loss: 0.6960467602462445\n",
            "loss: 0.695384324279192\n",
            "loss: 0.6961930429016497\n",
            "[1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
            " 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0]\n"
          ]
        }
      ],
      "source": [
        "def predict(w, X):\n",
        "    ##################\n",
        "    #INSERT CODE HERE#\n",
        "    ##################\n",
        "    preds = np.dot(X,w) #multiply matrix with model\n",
        "    for i in range(0,preds.size): #loop through prediction array\n",
        "        if preds[i] < 0.5: #check to which label the value is closer\n",
        "            preds[i] = 0 #prediction is closer to 0 than to 1\n",
        "            continue\n",
        "        preds[i] = 1 #prediction is closer to 1 than to 0\n",
        "    return preds #return the prediction\n",
        "\n",
        "print(predict(learn_reg_ERM(X_train,y_train,100), X_test)) #print prediction\n",
        "print(y_test) #print actual label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltgVMtXIxF0R"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "#### 5.1 \n",
        "Train a linear model on the training data and classify all 187 test instances afterwards using the function predict. \n",
        "Please note that the given class labels are in the range $\\left \\{0,1 \\right \\}$, however the learning algorithm expects a label in the range of $\\left \\{-1,1 \\right \\}$. Then, compute the accuracy of your trained linear model on both the training and the test data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "LqdCXWWYxF0R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 2.031642824882133\n",
            "loss: 0.525\n",
            "loss: 0.525\n",
            "loss: 1.0000000000000036\n",
            "loss: 0.525\n",
            "loss: 0.525\n",
            "loss: 1.0000000000000013\n",
            "loss: 0.525\n",
            "loss: 0.525\n",
            "loss: 1.0\n",
            "loss: 0.5252236888679076\n",
            "loss: 0.52601683573058\n",
            "loss: 0.9187500000000007\n",
            "loss: 0.525\n",
            "loss: 0.52523966621622\n",
            "loss: 0.5675265750937862\n",
            "loss: 0.5353664662864208\n",
            "loss: 0.5359417983884318\n",
            "loss: 0.8000000000000004\n",
            "loss: 0.5282450762177258\n",
            "loss: 0.528401138478495\n",
            "loss: 0.5312770476954415\n",
            "loss: 0.5308966043217739\n",
            "loss: 0.5407247952638462\n",
            "loss: 0.5345816935019381\n",
            "loss: 0.5342413630261516\n",
            "loss: 0.575\n",
            "loss: 0.5481846911978995\n",
            "loss: 0.5373501186007384\n",
            "loss: 0.5382267449680069\n",
            "loss: 0.5418353896887125\n",
            "loss: 0.5379365293765395\n",
            "loss: 0.537762857150693\n",
            "loss: 0.5374609380090585\n",
            "loss: 0.5381396588255576\n",
            "loss: 0.5374142367069898\n",
            "loss: 0.5374393245023671\n",
            "loss: 0.5374293368677182\n",
            "loss: 0.5374298041727226\n",
            "loss: 0.537484810551079\n",
            "loss: 0.5374344679042256\n",
            "loss: 0.5374960558594546\n",
            "loss: 0.5373939978220027\n",
            "loss: 0.5374351040071697\n",
            "loss: 0.5374025741398163\n",
            "The accuracy with the linear model on the training data is: 0.7375\n",
            "The accuracy with the linear model on the test data is: 0.9197860962566845\n"
          ]
        }
      ],
      "source": [
        "##################\n",
        "#INSERT CODE HERE#\n",
        "##################\n",
        "def accuracy(y_pred,y_true):\n",
        "    misses = 0 #variable to count misses\n",
        "    for i in range(0,y_pred.size): #loop through prediction array\n",
        "        if y_pred[i] != y_true[i]: #check if the prediction is wrong\n",
        "            misses += 1 #prediction is wrong we count one miss\n",
        "    return (y_pred.size - misses) / y_pred.size #calculate accuracy\n",
        "\n",
        "linear_model = learn_reg_ERM(X_train,y_train,2) #train model\n",
        "lm_train_pred = predict(linear_model, X_train) #make prediction on training data\n",
        "lm_test_pred = predict(linear_model, X_test) #make prediction on test data\n",
        "\n",
        "#display accuracy\n",
        "print(\"The accuracy with the linear model on the training data is:\", accuracy(lm_train_pred,y_train))\n",
        "print(\"The accuracy with the linear model on the test data is:\", accuracy(lm_test_pred,y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFdpQNg3xF0S"
      },
      "source": [
        "#### 5.2\n",
        "Compare the accuracy of the linear model with the accuracy of a random forest and a decision tree on the training and test data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "l_u_jEmXxF0T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy with the linear model on the training data is: 0.7375\n",
            "The accuracy with the linear model on the test data is: 0.9197860962566845\n",
            "The accuracy with the RandomForest on the training data is: 0.9375\n",
            "The accuracy with the RandomForest on the test data is: 0.7754010695187166\n",
            "The accuracy with the DecisionTree on the training data is: 0.725\n",
            "The accuracy with the DecisionTree on the test data is: 0.6149732620320856\n"
          ]
        }
      ],
      "source": [
        "##################\n",
        "#INSERT CODE HERE#\n",
        "##################\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#make predictions with RandomForest model\n",
        "forest = RandomForestClassifier()\n",
        "forest.fit(X_train,y_train) #train model\n",
        "forest_train_pred = forest.predict(X_train) #prediction on training data\n",
        "forest_test_pred = forest.predict(X_test) #prediction on test data\n",
        "\n",
        "#make predictions with DecisionTree model\n",
        "tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1)\n",
        "tree.fit(X_train,y_train) #train model\n",
        "tree_train_pred = tree.predict(X_train) #prediction on training data\n",
        "tree_test_pred = tree.predict(X_test) #prediction on test data\n",
        "\n",
        "#accuracy of linear model\n",
        "print(\"The accuracy with the linear model on the training data is:\", accuracy(lm_train_pred,y_train))\n",
        "print(\"The accuracy with the linear model on the test data is:\", accuracy(lm_test_pred,y_test))\n",
        "\n",
        "#accuracy of RandomForest model\n",
        "print(\"The accuracy with the RandomForest on the training data is:\", accuracy(forest_train_pred,y_train))\n",
        "print(\"The accuracy with the RandomForest on the test data is:\", accuracy(forest_test_pred,y_test))\n",
        "\n",
        "#accuracy of DecisionTree model\n",
        "print(\"The accuracy with the DecisionTree on the training data is:\", accuracy(tree_train_pred,y_train))\n",
        "print(\"The accuracy with the DecisionTree on the test data is:\", accuracy(tree_test_pred,y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab06_LinearClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
